
==> Audit <==
|-----------|-----------------|----------|-------------------------|---------|---------------------|---------------------|
|  Command  |      Args       | Profile  |          User           | Version |     Start Time      |      End Time       |
|-----------|-----------------|----------|-------------------------|---------|---------------------|---------------------|
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:32 IST |                     |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:35 IST |                     |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:43 IST |                     |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:45 IST |                     |
| start     | --driver=docker | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:52 IST |                     |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:52 IST |                     |
| delete    |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 09:54 IST | 25 Nov 24 09:54 IST |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 10:06 IST | 25 Nov 24 10:13 IST |
| kubectl   | -- get po -A    | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 10:15 IST | 25 Nov 24 10:16 IST |
| dashboard |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 10:17 IST |                     |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 12:46 IST | 25 Nov 24 12:47 IST |
| service   | ml-app-service  | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 25 Nov 24 12:59 IST | 25 Nov 24 13:00 IST |
| start     |                 | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 27 Dec 24 20:01 IST | 27 Dec 24 20:01 IST |
| service   | django-service  | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 27 Dec 24 20:27 IST |                     |
| service   | django-service  | minikube | LAPTOP-D7H8ORN5\chethan | v1.34.0 | 27 Dec 24 20:34 IST |                     |
|-----------|-----------------|----------|-------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/12/27 20:01:24
Running on machine: LAPTOP-D7H8ORN5
Binary: Built with gc go1.22.5 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1227 20:01:24.079056    1768 out.go:345] Setting OutFile to fd 96 ...
I1227 20:01:24.080169    1768 out.go:397] isatty.IsTerminal(96) = true
I1227 20:01:24.080169    1768 out.go:358] Setting ErrFile to fd 100...
I1227 20:01:24.080169    1768 out.go:397] isatty.IsTerminal(100) = true
I1227 20:01:24.102886    1768 out.go:352] Setting JSON to false
I1227 20:01:24.108498    1768 start.go:129] hostinfo: {"hostname":"LAPTOP-D7H8ORN5","uptime":1399951,"bootTime":1733909932,"procs":284,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4602 Build 22631.4602","kernelVersion":"10.0.22631.4602 Build 22631.4602","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"a4f36e07-7195-420d-a293-9f1390f35905"}
W1227 20:01:24.108498    1768 start.go:137] gopshost.Virtualization returned error: not implemented yet
I1227 20:01:24.111495    1768 out.go:177] ðŸ˜„  minikube v1.34.0 on Microsoft Windows 11 Home Single Language 10.0.22631.4602 Build 22631.4602
I1227 20:01:24.113887    1768 notify.go:220] Checking for updates...
I1227 20:01:24.114423    1768 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1227 20:01:24.114423    1768 driver.go:394] Setting default libvirt URI to qemu:///system
I1227 20:01:24.176832    1768 docker.go:123] docker version: linux-27.3.1:Docker Desktop 4.35.1 (173168)
I1227 20:01:24.181113    1768 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1227 20:01:24.230123    1768 info.go:266] docker info: {ID:4359b4c7-a95d-4828-9275-c6563c2313d1 Containers:25 ContainersRunning:1 ContainersPaused:0 ContainersStopped:24 Images:47 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:83 SystemTime:2024-12-27 10:18:51.707124291 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8159166464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I1227 20:01:24.232274    1768 out.go:177] âœ¨  Using the docker driver based on existing profile
I1227 20:01:24.234502    1768 start.go:297] selected driver: docker
I1227 20:01:24.234502    1768 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\chethan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1227 20:01:24.235022    1768 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1227 20:01:24.244556    1768 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1227 20:01:24.296232    1768 info.go:266] docker info: {ID:4359b4c7-a95d-4828-9275-c6563c2313d1 Containers:25 ContainersRunning:1 ContainersPaused:0 ContainersStopped:24 Images:47 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:55 OomKillDisable:true NGoroutines:83 SystemTime:2024-12-27 10:18:51.707124291 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:14 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8159166464 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.3.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:472731909fa34bd7bc9c087e4c27943f9835f111 Expected:472731909fa34bd7bc9c087e4c27943f9835f111} RuncCommit:{ID:v1.1.13-0-g58aa920 Expected:v1.1.13-0-g58aa920} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[] Warnings:<nil>}}
I1227 20:01:24.346681    1768 cni.go:84] Creating CNI manager for ""
I1227 20:01:24.346681    1768 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1227 20:01:24.346681    1768 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\chethan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1227 20:01:24.348567    1768 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1227 20:01:24.350673    1768 cache.go:121] Beginning downloading kic base image for docker with docker
I1227 20:01:24.352370    1768 out.go:177] ðŸšœ  Pulling base image v0.0.45 ...
I1227 20:01:24.354528    1768 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local docker daemon
I1227 20:01:24.354528    1768 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1227 20:01:24.355066    1768 preload.go:146] Found local preload: C:\Users\chethan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4
I1227 20:01:24.355066    1768 cache.go:56] Caching tarball of preloaded images
I1227 20:01:24.355245    1768 preload.go:172] Found C:\Users\chethan\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.31.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1227 20:01:24.355245    1768 cache.go:59] Finished verifying existence of preloaded tar for v1.31.0 on docker
I1227 20:01:24.355823    1768 profile.go:143] Saving config to C:\Users\chethan\.minikube\profiles\minikube\config.json ...
I1227 20:01:24.406550    1768 cache.go:149] Downloading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 to local cache
I1227 20:01:24.407119    1768 localpath.go:151] windows sanitize: C:\Users\chethan\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\chethan\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1227 20:01:24.407119    1768 localpath.go:151] windows sanitize: C:\Users\chethan\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\chethan\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1227 20:01:24.407119    1768 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory
I1227 20:01:24.407648    1768 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 in local cache directory, skipping pull
I1227 20:01:24.407648    1768 image.go:135] gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 exists in cache, skipping pull
I1227 20:01:24.407648    1768 cache.go:152] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 as a tarball
I1227 20:01:24.407648    1768 cache.go:162] Loading gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from local cache
I1227 20:01:24.407648    1768 localpath.go:151] windows sanitize: C:\Users\chethan\.minikube\cache\kic\amd64\kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar -> C:\Users\chethan\.minikube\cache\kic\amd64\kicbase_v0.0.45@sha256_81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85.tar
I1227 20:01:38.648789    1768 cache.go:164] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 from cached tarball
I1227 20:01:38.648789    1768 cache.go:194] Successfully downloaded all kic artifacts
I1227 20:01:38.649313    1768 start.go:360] acquireMachinesLock for minikube: {Name:mk7fb9f67e0143a02d04dbc4351a04e7e52fcfe9 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1227 20:01:38.649313    1768 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I1227 20:01:38.649831    1768 start.go:96] Skipping create...Using existing machine configuration
I1227 20:01:38.649831    1768 fix.go:54] fixHost starting: 
I1227 20:01:38.665420    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:38.732176    1768 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1227 20:01:38.732176    1768 fix.go:138] unexpected machine state, will restart: <nil>
I1227 20:01:38.733822    1768 out.go:177] ðŸ”„  Restarting existing docker container for "minikube" ...
I1227 20:01:38.741408    1768 cli_runner.go:164] Run: docker start minikube
I1227 20:01:39.496721    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:39.552440    1768 kic.go:430] container "minikube" state is running.
I1227 20:01:39.560585    1768 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 20:01:39.671041    1768 profile.go:143] Saving config to C:\Users\chethan\.minikube\profiles\minikube\config.json ...
I1227 20:01:39.672098    1768 machine.go:93] provisionDockerMachine start ...
I1227 20:01:39.678744    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:39.745624    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:39.746148    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:39.746148    1768 main.go:141] libmachine: About to run SSH command:
hostname
I1227 20:01:39.748755    1768 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I1227 20:01:42.929069    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1227 20:01:42.929069    1768 ubuntu.go:169] provisioning hostname "minikube"
I1227 20:01:42.936925    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:42.992314    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:42.992314    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:42.992314    1768 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1227 20:01:43.166360    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1227 20:01:43.171612    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:43.223895    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:43.223895    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:43.223895    1768 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1227 20:01:43.361458    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1227 20:01:43.361458    1768 ubuntu.go:175] set auth options {CertDir:C:\Users\chethan\.minikube CaCertPath:C:\Users\chethan\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\chethan\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\chethan\.minikube\machines\server.pem ServerKeyPath:C:\Users\chethan\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\chethan\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\chethan\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\chethan\.minikube}
I1227 20:01:43.361458    1768 ubuntu.go:177] setting up certificates
I1227 20:01:43.361458    1768 provision.go:84] configureAuth start
I1227 20:01:43.369383    1768 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 20:01:43.415678    1768 provision.go:143] copyHostCerts
I1227 20:01:43.422876    1768 exec_runner.go:144] found C:\Users\chethan\.minikube/ca.pem, removing ...
I1227 20:01:43.422876    1768 exec_runner.go:203] rm: C:\Users\chethan\.minikube\ca.pem
I1227 20:01:43.422876    1768 exec_runner.go:151] cp: C:\Users\chethan\.minikube\certs\ca.pem --> C:\Users\chethan\.minikube/ca.pem (1078 bytes)
I1227 20:01:43.423944    1768 exec_runner.go:144] found C:\Users\chethan\.minikube/cert.pem, removing ...
I1227 20:01:43.423944    1768 exec_runner.go:203] rm: C:\Users\chethan\.minikube\cert.pem
I1227 20:01:43.423944    1768 exec_runner.go:151] cp: C:\Users\chethan\.minikube\certs\cert.pem --> C:\Users\chethan\.minikube/cert.pem (1123 bytes)
I1227 20:01:43.424513    1768 exec_runner.go:144] found C:\Users\chethan\.minikube/key.pem, removing ...
I1227 20:01:43.424513    1768 exec_runner.go:203] rm: C:\Users\chethan\.minikube\key.pem
I1227 20:01:43.424513    1768 exec_runner.go:151] cp: C:\Users\chethan\.minikube\certs\key.pem --> C:\Users\chethan\.minikube/key.pem (1679 bytes)
I1227 20:01:43.425033    1768 provision.go:117] generating server cert: C:\Users\chethan\.minikube\machines\server.pem ca-key=C:\Users\chethan\.minikube\certs\ca.pem private-key=C:\Users\chethan\.minikube\certs\ca-key.pem org=chethan.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1227 20:01:43.496140    1768 provision.go:177] copyRemoteCerts
I1227 20:01:43.502169    1768 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1227 20:01:43.506431    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:43.554955    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:43.653830    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1227 20:01:43.677800    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I1227 20:01:43.696092    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1227 20:01:43.714730    1768 provision.go:87] duration metric: took 353.2727ms to configureAuth
I1227 20:01:43.714730    1768 ubuntu.go:193] setting minikube options for container-runtime
I1227 20:01:43.714730    1768 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1227 20:01:43.719434    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:43.772240    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:43.772240    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:43.772240    1768 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1227 20:01:43.908427    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1227 20:01:43.908427    1768 ubuntu.go:71] root file system type: overlay
I1227 20:01:43.908937    1768 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1227 20:01:43.914351    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:43.967616    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:43.968123    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:43.968123    1768 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1227 20:01:44.118384    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1227 20:01:44.123434    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:44.177548    1768 main.go:141] libmachine: Using SSH client type: native
I1227 20:01:44.178232    1768 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x6cc9c0] 0x6cf5a0 <nil>  [] 0s} 127.0.0.1 64383 <nil> <nil>}
I1227 20:01:44.178232    1768 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1227 20:01:44.326524    1768 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1227 20:01:44.326524    1768 machine.go:96] duration metric: took 4.6544263s to provisionDockerMachine
I1227 20:01:44.326524    1768 start.go:293] postStartSetup for "minikube" (driver="docker")
I1227 20:01:44.326524    1768 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1227 20:01:44.333859    1768 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1227 20:01:44.338502    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:44.403289    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:44.524151    1768 ssh_runner.go:195] Run: cat /etc/os-release
I1227 20:01:44.528996    1768 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1227 20:01:44.528996    1768 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1227 20:01:44.528996    1768 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1227 20:01:44.528996    1768 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I1227 20:01:44.529521    1768 filesync.go:126] Scanning C:\Users\chethan\.minikube\addons for local assets ...
I1227 20:01:44.529521    1768 filesync.go:126] Scanning C:\Users\chethan\.minikube\files for local assets ...
I1227 20:01:44.529521    1768 start.go:296] duration metric: took 202.9972ms for postStartSetup
I1227 20:01:44.536505    1768 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1227 20:01:44.540965    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:44.591352    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:44.701992    1768 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1227 20:01:44.707029    1768 fix.go:56] duration metric: took 6.0571981s for fixHost
I1227 20:01:44.707029    1768 start.go:83] releasing machines lock for "minikube", held for 6.0577158s
I1227 20:01:44.711713    1768 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1227 20:01:44.763382    1768 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I1227 20:01:44.768722    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:44.769815    1768 ssh_runner.go:195] Run: cat /version.json
I1227 20:01:44.775548    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:44.822027    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:44.824759    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
W1227 20:01:44.929637    1768 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I1227 20:01:44.939695    1768 ssh_runner.go:195] Run: systemctl --version
I1227 20:01:44.955158    1768 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1227 20:01:44.966013    1768 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W1227 20:01:44.977094    1768 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I1227 20:01:44.982995    1768 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1227 20:01:44.991845    1768 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1227 20:01:44.991845    1768 start.go:495] detecting cgroup driver to use...
I1227 20:01:44.991845    1768 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1227 20:01:44.991845    1768 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1227 20:01:45.009648    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I1227 20:01:45.026289    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1227 20:01:45.036738    1768 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I1227 20:01:45.043670    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I1227 20:01:45.059061    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1227 20:01:45.073689    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1227 20:01:45.088317    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1227 20:01:45.104228    1768 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1227 20:01:45.119390    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1227 20:01:45.136179    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1227 20:01:45.149980    1768 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1227 20:01:45.165262    1768 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1227 20:01:45.180161    1768 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1227 20:01:45.196038    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W1227 20:01:45.255192    1768 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W1227 20:01:45.255192    1768 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I1227 20:01:45.284059    1768 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1227 20:01:45.383445    1768 start.go:495] detecting cgroup driver to use...
I1227 20:01:45.383445    1768 detect.go:187] detected "cgroupfs" cgroup driver on host os
I1227 20:01:45.391433    1768 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1227 20:01:45.402042    1768 cruntime.go:279] skipping containerd shutdown because we are bound to it
I1227 20:01:45.408493    1768 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1227 20:01:45.420431    1768 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1227 20:01:45.438091    1768 ssh_runner.go:195] Run: which cri-dockerd
I1227 20:01:45.448703    1768 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1227 20:01:45.457320    1768 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I1227 20:01:45.483685    1768 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1227 20:01:45.588676    1768 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1227 20:01:45.673569    1768 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I1227 20:01:45.674109    1768 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I1227 20:01:45.693281    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 20:01:45.783145    1768 ssh_runner.go:195] Run: sudo systemctl restart docker
I1227 20:01:46.252806    1768 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1227 20:01:46.269052    1768 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I1227 20:01:46.288016    1768 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1227 20:01:46.303453    1768 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1227 20:01:46.399464    1768 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1227 20:01:46.496986    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 20:01:46.583274    1768 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1227 20:01:46.600987    1768 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1227 20:01:46.618067    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 20:01:46.700156    1768 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1227 20:01:47.000168    1768 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1227 20:01:47.006437    1768 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1227 20:01:47.010293    1768 start.go:563] Will wait 60s for crictl version
I1227 20:01:47.015611    1768 ssh_runner.go:195] Run: which crictl
I1227 20:01:47.024968    1768 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1227 20:01:47.164129    1768 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.2.0
RuntimeApiVersion:  v1
I1227 20:01:47.168402    1768 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1227 20:01:47.251134    1768 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1227 20:01:47.272106    1768 out.go:235] ðŸ³  Preparing Kubernetes v1.31.0 on Docker 27.2.0 ...
I1227 20:01:47.275932    1768 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I1227 20:01:47.396203    1768 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I1227 20:01:47.403091    1768 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I1227 20:01:47.407219    1768 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1227 20:01:47.422128    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1227 20:01:47.469360    1768 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\chethan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1227 20:01:47.469360    1768 preload.go:131] Checking if preload exists for k8s version v1.31.0 and runtime docker
I1227 20:01:47.474165    1768 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1227 20:01:47.496824    1768 docker.go:685] Got preloaded images: -- stdout --
chethancv/ml-app:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1227 20:01:47.496824    1768 docker.go:615] Images already preloaded, skipping extraction
I1227 20:01:47.502040    1768 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1227 20:01:47.519232    1768 docker.go:685] Got preloaded images: -- stdout --
chethancv/ml-app:latest
registry.k8s.io/kube-controller-manager:v1.31.0
registry.k8s.io/kube-apiserver:v1.31.0
registry.k8s.io/kube-scheduler:v1.31.0
registry.k8s.io/kube-proxy:v1.31.0
registry.k8s.io/etcd:3.5.15-0
registry.k8s.io/pause:3.10
registry.k8s.io/coredns/coredns:v1.11.1
kubernetesui/dashboard:<none>
kubernetesui/metrics-scraper:<none>
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1227 20:01:47.519232    1768 cache_images.go:84] Images are preloaded, skipping loading
I1227 20:01:47.519232    1768 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.31.0 docker true true} ...
I1227 20:01:47.519232    1768 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.31.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1227 20:01:47.525354    1768 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1227 20:01:47.870373    1768 cni.go:84] Creating CNI manager for ""
I1227 20:01:47.870373    1768 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1227 20:01:47.870373    1768 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1227 20:01:47.870373    1768 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.31.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1227 20:01:47.870373    1768 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.31.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1227 20:01:47.875887    1768 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.31.0
I1227 20:01:47.885053    1768 binaries.go:44] Found k8s binaries, skipping transfer
I1227 20:01:47.890953    1768 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1227 20:01:47.899303    1768 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1227 20:01:47.912490    1768 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1227 20:01:47.925381    1768 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I1227 20:01:47.944747    1768 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1227 20:01:47.949115    1768 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1227 20:01:47.965716    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 20:01:48.069989    1768 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1227 20:01:48.082196    1768 certs.go:68] Setting up C:\Users\chethan\.minikube\profiles\minikube for IP: 192.168.49.2
I1227 20:01:48.082196    1768 certs.go:194] generating shared ca certs ...
I1227 20:01:48.082196    1768 certs.go:226] acquiring lock for ca certs: {Name:mkc3c5d08ae050702b6d8c98c58dd77ace54dcd1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 20:01:48.082714    1768 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\chethan\.minikube\ca.key
I1227 20:01:48.089911    1768 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\chethan\.minikube\proxy-client-ca.key
I1227 20:01:48.090002    1768 certs.go:256] generating profile certs ...
I1227 20:01:48.090002    1768 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\chethan\.minikube\profiles\minikube\client.key
I1227 20:01:48.090507    1768 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\chethan\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I1227 20:01:48.091102    1768 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\chethan\.minikube\profiles\minikube\proxy-client.key
I1227 20:01:48.091794    1768 certs.go:484] found cert: C:\Users\chethan\.minikube\certs\ca-key.pem (1675 bytes)
I1227 20:01:48.091913    1768 certs.go:484] found cert: C:\Users\chethan\.minikube\certs\ca.pem (1078 bytes)
I1227 20:01:48.091913    1768 certs.go:484] found cert: C:\Users\chethan\.minikube\certs\cert.pem (1123 bytes)
I1227 20:01:48.092034    1768 certs.go:484] found cert: C:\Users\chethan\.minikube\certs\key.pem (1679 bytes)
I1227 20:01:48.092427    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1227 20:01:48.111125    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I1227 20:01:48.128701    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1227 20:01:48.146558    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1227 20:01:48.165841    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1227 20:01:48.186330    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1227 20:01:48.204554    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1227 20:01:48.222273    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1227 20:01:48.241236    1768 ssh_runner.go:362] scp C:\Users\chethan\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1227 20:01:48.260204    1768 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1227 20:01:48.281952    1768 ssh_runner.go:195] Run: openssl version
I1227 20:01:48.295416    1768 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1227 20:01:48.312552    1768 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1227 20:01:48.316511    1768 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 25 04:42 /usr/share/ca-certificates/minikubeCA.pem
I1227 20:01:48.321834    1768 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1227 20:01:48.333560    1768 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1227 20:01:48.350969    1768 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1227 20:01:48.367960    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1227 20:01:48.383830    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1227 20:01:48.398695    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1227 20:01:48.460343    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1227 20:01:48.474647    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1227 20:01:48.490028    1768 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1227 20:01:48.497460    1768 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.45@sha256:81df288595202a317b1a4dc2506ca2e4ed5f22373c19a441b88cfbf4b9867c85 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.31.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\chethan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1227 20:01:48.502381    1768 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1227 20:01:48.528328    1768 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1227 20:01:48.548465    1768 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I1227 20:01:48.548465    1768 kubeadm.go:593] restartPrimaryControlPlane start ...
I1227 20:01:48.556437    1768 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1227 20:01:48.565489    1768 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1227 20:01:48.575802    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1227 20:01:48.622777    1768 kubeconfig.go:125] found "minikube" server: "https://127.0.0.1:53333"
I1227 20:01:48.622777    1768 kubeconfig.go:47] verify endpoint returned: got: 127.0.0.1:53333, want: 127.0.0.1:64387
I1227 20:01:48.622777    1768 kubeconfig.go:62] C:\Users\chethan\.kube\config needs updating (will repair): [kubeconfig needs server address update]
I1227 20:01:48.623298    1768 lock.go:35] WriteFile acquiring C:\Users\chethan\.kube\config: {Name:mk4ca41cf61cc553edcff1698704ae8469fe2278 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 20:01:48.631159    1768 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1227 20:01:48.641936    1768 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I1227 20:01:48.641936    1768 kubeadm.go:597] duration metric: took 93.4711ms to restartPrimaryControlPlane
I1227 20:01:48.641936    1768 kubeadm.go:394] duration metric: took 144.4757ms to StartCluster
I1227 20:01:48.641936    1768 settings.go:142] acquiring lock: {Name:mk848e6069b4f6e2a0e294c066f6836d4a513934 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 20:01:48.641936    1768 settings.go:150] Updating kubeconfig:  C:\Users\chethan\.kube\config
I1227 20:01:48.642999    1768 lock.go:35] WriteFile acquiring C:\Users\chethan\.kube\config: {Name:mk4ca41cf61cc553edcff1698704ae8469fe2278 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1227 20:01:48.643526    1768 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.31.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1227 20:01:48.643526    1768 addons.go:507] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1227 20:01:48.643526    1768 addons.go:69] Setting dashboard=true in profile "minikube"
I1227 20:01:48.643526    1768 addons.go:234] Setting addon dashboard=true in "minikube"
W1227 20:01:48.643526    1768 addons.go:243] addon dashboard should already be in state true
I1227 20:01:48.643526    1768 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.31.0
I1227 20:01:48.643526    1768 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1227 20:01:48.644139    1768 host.go:66] Checking if "minikube" exists ...
I1227 20:01:48.644139    1768 addons.go:234] Setting addon storage-provisioner=true in "minikube"
I1227 20:01:48.643526    1768 addons.go:69] Setting default-storageclass=true in profile "minikube"
W1227 20:01:48.644139    1768 addons.go:243] addon storage-provisioner should already be in state true
I1227 20:01:48.644139    1768 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1227 20:01:48.644139    1768 host.go:66] Checking if "minikube" exists ...
I1227 20:01:48.645701    1768 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I1227 20:01:48.658146    1768 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1227 20:01:48.658655    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:48.659701    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:48.659701    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:48.721438    1768 out.go:177]     â–ª Using image docker.io/kubernetesui/dashboard:v2.7.0
I1227 20:01:48.733622    1768 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1227 20:01:48.726902    1768 addons.go:234] Setting addon default-storageclass=true in "minikube"
W1227 20:01:48.733622    1768 addons.go:243] addon default-storageclass should already be in state true
I1227 20:01:48.733622    1768 host.go:66] Checking if "minikube" exists ...
I1227 20:01:48.747911    1768 addons.go:431] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1227 20:01:48.747911    1768 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1227 20:01:48.748430    1768 out.go:177]     â–ª Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I1227 20:01:48.750503    1768 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1227 20:01:48.751552    1768 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1227 20:01:48.753122    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:48.754996    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-ns.yaml
I1227 20:01:48.755528    1768 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I1227 20:01:48.765747    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:48.770798    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I1227 20:01:48.810839    1768 addons.go:431] installing /etc/kubernetes/addons/storageclass.yaml
I1227 20:01:48.810839    1768 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1227 20:01:48.817915    1768 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1227 20:01:48.824480    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:48.831501    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:48.832580    1768 api_server.go:52] waiting for apiserver process to appear ...
I1227 20:01:48.839682    1768 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1227 20:01:48.869149    1768 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:64383 SSHKeyPath:C:\Users\chethan\.minikube\machines\minikube\id_rsa Username:docker}
I1227 20:01:48.950903    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I1227 20:01:48.950903    1768 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I1227 20:01:48.959528    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1227 20:01:48.969160    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I1227 20:01:48.969160    1768 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I1227 20:01:48.986953    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1227 20:01:48.987476    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I1227 20:01:48.987476    1768 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I1227 20:01:49.003631    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-dp.yaml
I1227 20:01:49.003631    1768 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I1227 20:01:49.020338    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-role.yaml
I1227 20:01:49.020338    1768 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I1227 20:01:49.049559    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I1227 20:01:49.049559    1768 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I1227 20:01:49.065465    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-sa.yaml
I1227 20:01:49.065465    1768 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I1227 20:01:49.080928    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-secret.yaml
I1227 20:01:49.080928    1768 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I1227 20:01:49.095633    1768 addons.go:431] installing /etc/kubernetes/addons/dashboard-svc.yaml
I1227 20:01:49.095633    1768 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I1227 20:01:49.122368    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1227 20:01:49.247627    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1227 20:01:49.247627    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.247627    1768 retry.go:31] will retry after 367.29525ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.247627    1768 retry.go:31] will retry after 240.015325ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1227 20:01:49.348726    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.348726    1768 retry.go:31] will retry after 182.308433ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.361044    1768 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1227 20:01:49.498272    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1227 20:01:49.544662    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1227 20:01:49.564294    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.564799    1768 retry.go:31] will retry after 208.771689ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.629011    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W1227 20:01:49.764143    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.764143    1768 retry.go:31] will retry after 259.638472ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.788313    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1227 20:01:49.855355    1768 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1227 20:01:49.948300    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:49.948300    1768 retry.go:31] will retry after 394.675779ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.035969    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
W1227 20:01:50.061379    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.061379    1768 retry.go:31] will retry after 656.151754ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W1227 20:01:50.161722    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.161722    1768 retry.go:31] will retry after 615.98452ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: Process exited with status 1
stdout:

stderr:
error validating "/etc/kubernetes/addons/dashboard-ns.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrole.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-clusterrolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-configmap.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-dp.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-role.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-rolebinding.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-sa.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-secret.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
error validating "/etc/kubernetes/addons/dashboard-svc.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.350661    1768 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1227 20:01:50.359849    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1227 20:01:50.363017    1768 api_server.go:72] duration metric: took 1.7194913s to wait for apiserver process to appear ...
I1227 20:01:50.363017    1768 api_server.go:88] waiting for apiserver healthz status ...
I1227 20:01:50.363017    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:50.365414    1768 api_server.go:269] stopped: https://127.0.0.1:64387/healthz: Get "https://127.0.0.1:64387/healthz": EOF
W1227 20:01:50.449800    1768 addons.go:457] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.449800    1768 retry.go:31] will retry after 360.154132ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I1227 20:01:50.726695    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I1227 20:01:50.789259    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I1227 20:01:50.832420    1768 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I1227 20:01:50.869449    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:52.796092    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1227 20:01:52.796092    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1227 20:01:52.796092    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:52.800725    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1227 20:01:52.800725    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1227 20:01:52.869412    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:52.958945    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1227 20:01:52.958945    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1227 20:01:53.369563    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:53.374819    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1227 20:01:53.374819    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1227 20:01:53.863902    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:53.869612    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1227 20:01:53.869612    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1227 20:01:54.377434    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:54.450103    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W1227 20:01:54.450103    1768 api_server.go:103] status: https://127.0.0.1:64387/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I1227 20:01:54.453965    1768 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.7272698s)
I1227 20:01:54.874558    1768 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:64387/healthz ...
I1227 20:01:54.948849    1768 api_server.go:279] https://127.0.0.1:64387/healthz returned 200:
ok
I1227 20:01:54.960856    1768 api_server.go:141] control plane version: v1.31.0
I1227 20:01:54.960856    1768 api_server.go:131] duration metric: took 4.5978389s to wait for apiserver health ...
I1227 20:01:54.960856    1768 system_pods.go:43] waiting for kube-system pods to appear ...
I1227 20:01:54.969908    1768 system_pods.go:59] 7 kube-system pods found
I1227 20:01:54.969908    1768 system_pods.go:61] "coredns-6f6b679f8f-7m4mv" [9ecaa761-0717-4ebd-b95d-0b72d0d3effe] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1227 20:01:54.969908    1768 system_pods.go:61] "etcd-minikube" [81e97fb9-b47a-4d45-8c69-39ef5bd174b1] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I1227 20:01:54.969908    1768 system_pods.go:61] "kube-apiserver-minikube" [9515f9a3-4a55-4fc1-b003-4670e5329e65] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1227 20:01:54.969908    1768 system_pods.go:61] "kube-controller-manager-minikube" [a6b7daab-9a0c-437f-9bb2-db5022d885dd] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1227 20:01:54.969908    1768 system_pods.go:61] "kube-proxy-54f4s" [c1f90920-1919-4692-8b30-4da910da67c7] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1227 20:01:54.969908    1768 system_pods.go:61] "kube-scheduler-minikube" [ef9f7fc9-4d57-48ab-b7dd-959125fcd934] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1227 20:01:54.969908    1768 system_pods.go:61] "storage-provisioner" [1ea4fe10-6fb0-407a-ac75-7644c9223ed2] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1227 20:01:54.969908    1768 system_pods.go:74] duration metric: took 9.0514ms to wait for pod list to return data ...
I1227 20:01:54.969908    1768 kubeadm.go:582] duration metric: took 6.3263816s to wait for: map[apiserver:true system_pods:true]
I1227 20:01:54.969908    1768 node_conditions.go:102] verifying NodePressure condition ...
I1227 20:01:55.049911    1768 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I1227 20:01:55.049911    1768 node_conditions.go:123] node cpu capacity is 12
I1227 20:01:55.049911    1768 node_conditions.go:105] duration metric: took 80.0037ms to run NodePressure ...
I1227 20:01:55.049911    1768 start.go:241] waiting for startup goroutines ...
I1227 20:01:57.352295    1768 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (6.519875s)
I1227 20:01:57.352295    1768 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.31.0/kubectl apply --force -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (6.5630361s)
I1227 20:01:57.354979    1768 out.go:177] ðŸ’¡  Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I1227 20:01:57.367696    1768 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, dashboard, default-storageclass
I1227 20:01:57.370354    1768 addons.go:510] duration metric: took 8.7268276s for enable addons: enabled=[storage-provisioner dashboard default-storageclass]
I1227 20:01:57.370354    1768 start.go:246] waiting for cluster config update ...
I1227 20:01:57.370354    1768 start.go:255] writing updated cluster config ...
I1227 20:01:57.376804    1768 ssh_runner.go:195] Run: rm -f paused
I1227 20:01:57.723325    1768 start.go:600] kubectl: 1.30.2, cluster: 1.31.0 (minor skew: 1)
I1227 20:01:57.724966    1768 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186624712Z" level=warning msg="error locating sandbox id 22e1b2362d8ab32cd0e58ede0a02304891f45fbf0a0c4dcddde2bcb2007977c7: sandbox 22e1b2362d8ab32cd0e58ede0a02304891f45fbf0a0c4dcddde2bcb2007977c7 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186633518Z" level=warning msg="error locating sandbox id 39aca4eb341bf1280574473ee03450d7b677366c1da21aa67f5f40d29680f10c: sandbox 39aca4eb341bf1280574473ee03450d7b677366c1da21aa67f5f40d29680f10c not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186642529Z" level=warning msg="error locating sandbox id 8128450a4829ff042fe9f8f709e9c1d627f7bf8836d68ef23d2580aa756d1076: sandbox 8128450a4829ff042fe9f8f709e9c1d627f7bf8836d68ef23d2580aa756d1076 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186650394Z" level=warning msg="error locating sandbox id 834ec7d744fe1644fb14d32fb3eb651fdeb8f0784a308417fdf06e6a208dffb9: sandbox 834ec7d744fe1644fb14d32fb3eb651fdeb8f0784a308417fdf06e6a208dffb9 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186657384Z" level=warning msg="error locating sandbox id 3218c7c64dd747d675e87b23008ec19045daf10a6e9b97c7b8aa74ed4b036a27: sandbox 3218c7c64dd747d675e87b23008ec19045daf10a6e9b97c7b8aa74ed4b036a27 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186679806Z" level=warning msg="error locating sandbox id f840bbdbe9dbb2f86bdc16acd3ef03717601e0ecfb13bff6e73fc6e62d85aa21: sandbox f840bbdbe9dbb2f86bdc16acd3ef03717601e0ecfb13bff6e73fc6e62d85aa21 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186695818Z" level=warning msg="error locating sandbox id c16b754c2fdb833c2d24aaeefcec263f3fe861fb261beae6c12d2ae7a2d73b0e: sandbox c16b754c2fdb833c2d24aaeefcec263f3fe861fb261beae6c12d2ae7a2d73b0e not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186703849Z" level=warning msg="error locating sandbox id 2f24d6652ccf7f9bffcbaf1b73c2c776a831dcc5943c46b19e6d963a6ef9cdc5: sandbox 2f24d6652ccf7f9bffcbaf1b73c2c776a831dcc5943c46b19e6d963a6ef9cdc5 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186712677Z" level=warning msg="error locating sandbox id 164e0f1994557cde054af13a4b3f4c569ddb8f8c1dde4ac6683e3489bc220cc8: sandbox 164e0f1994557cde054af13a4b3f4c569ddb8f8c1dde4ac6683e3489bc220cc8 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186721568Z" level=warning msg="error locating sandbox id ba8316dfef7e9b9acc2fb458d8564be0b0d58ce14a7cd6c49cd04e64dabcc43b: sandbox ba8316dfef7e9b9acc2fb458d8564be0b0d58ce14a7cd6c49cd04e64dabcc43b not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186730601Z" level=warning msg="error locating sandbox id aeb3d230d8b535011ef7977dd2734d38d4b399580f542089d0703f02c3d45c15: sandbox aeb3d230d8b535011ef7977dd2734d38d4b399580f542089d0703f02c3d45c15 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186738845Z" level=warning msg="error locating sandbox id 92c154be0d5a62dc87affd19d4f07f7f9d2c94b6197a26e0ddf20b3d5a6f38d1: sandbox 92c154be0d5a62dc87affd19d4f07f7f9d2c94b6197a26e0ddf20b3d5a6f38d1 not found"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.186984316Z" level=info msg="Loading containers: done."
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.203983953Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.204035005Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.204039246Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.204042102Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.204064058Z" level=info msg="Docker daemon" commit=3ab5c7d containerd-snapshotter=false storage-driver=overlay2 version=27.2.0
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.204132584Z" level=info msg="Daemon has completed initialization"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.243917058Z" level=info msg="API listen on /var/run/docker.sock"
Dec 27 14:31:46 minikube dockerd[1132]: time="2024-12-27T14:31:46.243960425Z" level=info msg="API listen on [::]:2376"
Dec 27 14:31:46 minikube systemd[1]: Started Docker Application Container Engine.
Dec 27 14:31:46 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Start docker client with request timeout 0s"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Hairpin mode is set to hairpin-veth"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Loaded network plugin cni"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Docker cri networking managed by network plugin cni"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Setting cgroupDriver cgroupfs"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Dec 27 14:31:46 minikube cri-dockerd[1504]: time="2024-12-27T14:31:46Z" level=info msg="Start cri-dockerd grpc backend"
Dec 27 14:31:46 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"ml-app-7dc5bb847b-664h6_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"6c7c73fa23c0bcc5f8ea7ab46fa0889cb417d4463b6bfcfcf73f52aca9728af1\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-7m4mv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"02a570e69ad71041191c43281a3639ef802fb8b58c744f096c80da56429a5963\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-7m4mv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"a3991b034e061d52d8feac2a2a86e8ab51e0eeda029cf4e89b2beb07c3ae99d5\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-rtm4w_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ae6d839e50911edb85b85d4aabcf2275906d7b03804e88b3ff94a0d32e60f3ea\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-rtm4w_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"b57f5f3fbf5b8a7abba7cf9e515eb718aa4961a9ef6becb4732bcdd5258a1e77\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-4rq9k_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"705eb855ec2775063a93335271614f2d7edad2ff1cfe3026be3124ba2da17ab6\""
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"4105fd6ed0a5866aa0adbff0629021492283f86bf7d549bb48b97a5d61aeabc0\". Proceed without further sandbox information."
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"1a5afb48cc2cf657bfcd8d1aeb9a1bc392277dca4019998586581e1a6b3d020c\". Proceed without further sandbox information."
Dec 27 14:31:48 minikube cri-dockerd[1504]: time="2024-12-27T14:31:48Z" level=info msg="Both sandbox container and checkpoint could not be found with id \"164d6b794c22d83f8643080eb74db61929ce54b83657b66980e5486be615efc1\". Proceed without further sandbox information."
Dec 27 14:31:49 minikube cri-dockerd[1504]: time="2024-12-27T14:31:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ae6291d0f624114e16eb50b350a2ad6b4fd631db35b6fccd8d7a85a373374c3f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:49 minikube cri-dockerd[1504]: time="2024-12-27T14:31:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4fb5e6e78c9d49637302997558f869cedc6975479000efaed9c9c8017a224209/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:49 minikube cri-dockerd[1504]: time="2024-12-27T14:31:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f41582983b0a18b54ed9ad68e77cd4cd323211ff1d7d3a14af8a9aa56ffccd28/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:49 minikube cri-dockerd[1504]: time="2024-12-27T14:31:49Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/784db0dfeafe3da75a0c91026c85a30ba7a5fec9b72279b30fb20ea3fc6ebc6d/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:50 minikube cri-dockerd[1504]: time="2024-12-27T14:31:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-695b96c756-4rq9k_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"705eb855ec2775063a93335271614f2d7edad2ff1cfe3026be3124ba2da17ab6\""
Dec 27 14:31:50 minikube cri-dockerd[1504]: time="2024-12-27T14:31:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-c5db448b4-rtm4w_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"ae6d839e50911edb85b85d4aabcf2275906d7b03804e88b3ff94a0d32e60f3ea\""
Dec 27 14:31:50 minikube cri-dockerd[1504]: time="2024-12-27T14:31:50Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-6f6b679f8f-7m4mv_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"02a570e69ad71041191c43281a3639ef802fb8b58c744f096c80da56429a5963\""
Dec 27 14:31:53 minikube cri-dockerd[1504]: time="2024-12-27T14:31:53Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Dec 27 14:31:54 minikube cri-dockerd[1504]: time="2024-12-27T14:31:54Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9e6724f271e6add86bad667a262bfb2e81b862f0f73d76d6ebb60ba7012863ac/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:55 minikube cri-dockerd[1504]: time="2024-12-27T14:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e1a489ee169ed37154ebee8b09bd2188092e2f5849774bbb6b126a8dfa95669b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:55 minikube cri-dockerd[1504]: time="2024-12-27T14:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/78455f238720d1847acd4b64e0abbb0884b823d82424f40cd7035b53c3b4cfbb/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Dec 27 14:31:55 minikube cri-dockerd[1504]: time="2024-12-27T14:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/cba1a47aeb3d4d93d1983c3a399c8a9a7b68fb7332a929526bf511126e5632c1/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 27 14:31:55 minikube cri-dockerd[1504]: time="2024-12-27T14:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/64cf75b5519fedc7574e0c9eb9464fdfbace245205df179838353e035c7886fe/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 27 14:31:55 minikube cri-dockerd[1504]: time="2024-12-27T14:31:55Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2c85b15653df256fc97e26f7b46ff7bdc9701e4b5411f2bb08cc263e0366fc12/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Dec 27 14:31:59 minikube cri-dockerd[1504]: time="2024-12-27T14:31:59Z" level=info msg="Stop pulling image chethancv/ml-app:latest: Status: Image is up to date for chethancv/ml-app:latest"
Dec 27 14:32:17 minikube dockerd[1132]: time="2024-12-27T14:32:17.734542442Z" level=info msg="ignoring event" container=4c61eebac961c1a12c9d1d7b71b564ba5224289b22818c3ddeecf56fdddb2c91 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Dec 27 14:32:18 minikube dockerd[1132]: time="2024-12-27T14:32:18.723732274Z" level=info msg="ignoring event" container=241c2ac823bb9f383128434029e749333c16c0aef6d3b00e7c906cc774ddc177 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
c6ca97adf2cdb       07655ddf2eebe                                                                              34 minutes ago      Running             kubernetes-dashboard        4                   2c85b15653df2       kubernetes-dashboard-695b96c756-4rq9k
b8a793709c235       6e38f40d628db                                                                              34 minutes ago      Running             storage-provisioner         5                   e1a489ee169ed       storage-provisioner
9cbbfa9f06f54       chethancv/ml-app@sha256:156aa39157b3a263ef910949227c14bb4dcaa30fd482dab6f4eb1afe19ea20f0   34 minutes ago      Running             ml-app                      1                   cba1a47aeb3d4       ml-app-7dc5bb847b-664h6
241c2ac823bb9       07655ddf2eebe                                                                              34 minutes ago      Exited              kubernetes-dashboard        3                   2c85b15653df2       kubernetes-dashboard-695b96c756-4rq9k
e9e9a9db3dbff       115053965e86b                                                                              34 minutes ago      Running             dashboard-metrics-scraper   2                   64cf75b5519fe       dashboard-metrics-scraper-c5db448b4-rtm4w
4c61eebac961c       6e38f40d628db                                                                              34 minutes ago      Exited              storage-provisioner         4                   e1a489ee169ed       storage-provisioner
c264550275fbc       cbb01a7bd410d                                                                              34 minutes ago      Running             coredns                     2                   78455f238720d       coredns-6f6b679f8f-7m4mv
8f79b33606b8c       ad83b2ca7b09e                                                                              34 minutes ago      Running             kube-proxy                  2                   9e6724f271e6a       kube-proxy-54f4s
1e3d2cbf4aee9       604f5db92eaa8                                                                              34 minutes ago      Running             kube-apiserver              2                   ae6291d0f6241       kube-apiserver-minikube
f091bd1bd268d       1766f54c897f0                                                                              34 minutes ago      Running             kube-scheduler              2                   784db0dfeafe3       kube-scheduler-minikube
e373a5ba3131d       2e96e5913fc06                                                                              34 minutes ago      Running             etcd                        2                   f41582983b0a1       etcd-minikube
5549c79bad513       045733566833c                                                                              34 minutes ago      Running             kube-controller-manager     2                   4fb5e6e78c9d4       kube-controller-manager-minikube
321d50e609966       chethancv/ml-app@sha256:156aa39157b3a263ef910949227c14bb4dcaa30fd482dab6f4eb1afe19ea20f0   4 weeks ago         Exited              ml-app                      0                   6c7c73fa23c0b       ml-app-7dc5bb847b-664h6
5509ed995daaa       115053965e86b                                                                              4 weeks ago         Exited              dashboard-metrics-scraper   1                   ae6d839e50911       dashboard-metrics-scraper-c5db448b4-rtm4w
348f4ace8051b       ad83b2ca7b09e                                                                              4 weeks ago         Exited              kube-proxy                  1                   930707f149cc1       kube-proxy-54f4s
9ec21faef2596       cbb01a7bd410d                                                                              4 weeks ago         Exited              coredns                     1                   02a570e69ad71       coredns-6f6b679f8f-7m4mv
01b88254faa4d       604f5db92eaa8                                                                              4 weeks ago         Exited              kube-apiserver              1                   a0098a16b32bd       kube-apiserver-minikube
d975caf3b0e21       045733566833c                                                                              4 weeks ago         Exited              kube-controller-manager     1                   51671ad435d6d       kube-controller-manager-minikube
7fc4527ec12f8       1766f54c897f0                                                                              4 weeks ago         Exited              kube-scheduler              1                   baeac99d56cca       kube-scheduler-minikube
6b462c93e61bd       2e96e5913fc06                                                                              4 weeks ago         Exited              etcd                        1                   febe32c75844b       etcd-minikube


==> coredns [9ec21faef259] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:47693 - 54780 "HINFO IN 6360060859952123535.9078754385425439169. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.046250753s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1977081573]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Nov-2024 07:17:11.920) (total time: 21059ms):
Trace[1977081573]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (07:17:32.969)
Trace[1977081573]: [21.059533493s] [21.059533493s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[555290787]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Nov-2024 07:17:11.920) (total time: 21059ms):
Trace[555290787]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21049ms (07:17:32.969)
Trace[555290787]: [21.059681148s] [21.059681148s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1696688747]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (25-Nov-2024 07:17:11.920) (total time: 21059ms):
Trace[1696688747]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21049ms (07:17:32.969)
Trace[1696688747]: [21.059788596s] [21.059788596s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> coredns [c264550275fb] <==
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = f869070685748660180df1b7a47d58cdafcf2f368266578c062d1151dc2c900964aecc5975e8882e6de6fdfb6460463e30ebfaad2ec8f0c3c6436f80225b3b5b
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:51175 - 61338 "HINFO IN 2363828256050710950.3415398270878626678. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.27668627s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1479205930]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Dec-2024 14:31:57.565) (total time: 21051ms):
Trace[1479205930]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (14:32:18.614)
Trace[1479205930]: [21.051165666s] [21.051165666s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1504816742]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Dec-2024 14:31:57.565) (total time: 21051ms):
Trace[1504816742]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (14:32:18.614)
Trace[1504816742]: [21.051421835s] [21.051421835s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1932626896]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (27-Dec-2024 14:31:57.565) (total time: 21051ms):
Trace[1932626896]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21050ms (14:32:18.614)
Trace[1932626896]: [21.051568675s] [21.051568675s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=210b148df93a80eb872ecbeb7e35281b3c582c61
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_11_25T10_13_03_0700
                    minikube.k8s.io/version=v1.34.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 25 Nov 2024 04:42:59 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 27 Dec 2024 15:06:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 27 Dec 2024 15:02:32 +0000   Mon, 25 Nov 2024 04:42:58 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 27 Dec 2024 15:02:32 +0000   Mon, 25 Nov 2024 04:42:58 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 27 Dec 2024 15:02:32 +0000   Mon, 25 Nov 2024 04:42:58 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 27 Dec 2024 15:02:32 +0000   Mon, 25 Nov 2024 04:43:00 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7967936Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7967936Ki
  pods:               110
System Info:
  Machine ID:                 52d80507a5db46739302bd9a161dfeb4
  System UUID:                52d80507a5db46739302bd9a161dfeb4
  Boot ID:                    169a3b6a-fe0b-43fb-963c-8f94df79105c
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.2.0
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     django-app-7754657c8b-7xp9r                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m
  default                     django-app-7754657c8b-z5ckw                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m
  default                     ml-app-7dc5bb847b-664h6                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         32d
  kube-system                 coredns-6f6b679f8f-7m4mv                     100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     32d
  kube-system                 etcd-minikube                                100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         32d
  kube-system                 kube-apiserver-minikube                      250m (2%)     0 (0%)      0 (0%)           0 (0%)         32d
  kube-system                 kube-controller-manager-minikube             200m (1%)     0 (0%)      0 (0%)           0 (0%)         32d
  kube-system                 kube-proxy-54f4s                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         32d
  kube-system                 kube-scheduler-minikube                      100m (0%)     0 (0%)      0 (0%)           0 (0%)         32d
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         32d
  kubernetes-dashboard        dashboard-metrics-scraper-c5db448b4-rtm4w    0 (0%)        0 (0%)      0 (0%)           0 (0%)         32d
  kubernetes-dashboard        kubernetes-dashboard-695b96c756-4rq9k        0 (0%)        0 (0%)      0 (0%)           0 (0%)         32d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%)   0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           32d                kube-proxy       
  Normal   Starting                           34m                kube-proxy       
  Normal   Starting                           32d                kube-proxy       
  Normal   NodeAllocatableEnforced            32d                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           32d                kubelet          Starting kubelet.
  Warning  CgroupV1                           32d                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory            32d                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              32d                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               32d                kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  32d                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   RegisteredNode                     32d                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeHasSufficientMemory            32d (x7 over 32d)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   Starting                           32d                kubelet          Starting kubelet.
  Warning  CgroupV1                           32d                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Warning  PossibleMemoryBackedVolumesOnDisk  32d                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasNoDiskPressure              32d (x7 over 32d)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               32d (x7 over 32d)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            32d                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     32d                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  CgroupV1                           34m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   Starting                           34m                kubelet          Starting kubelet.
  Warning  PossibleMemoryBackedVolumesOnDisk  34m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   NodeHasSufficientMemory            34m (x7 over 34m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              34m (x7 over 34m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               34m (x7 over 34m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            34m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     34m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Dec27 12:25] PCI: Fatal: No config space access function found
[  +0.013019] PCI: System does not support PCI
[  +0.110093] kvm: already loaded the other module
[  +3.114848] FS-Cache: Duplicate cookie detected
[  +0.001194] FS-Cache: O-cookie c=00000005 [p=00000002 fl=222 nc=0 na=1]
[  +0.002278] FS-Cache: O-cookie d=0000000084819ec0{9P.session} n=0000000051891204
[  +0.000928] FS-Cache: O-key=[10] '34323934393337363231'
[  +0.000599] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000911] FS-Cache: N-cookie d=0000000084819ec0{9P.session} n=000000008b902e9b
[  +0.001031] FS-Cache: N-key=[10] '34323934393337363231'
[  +1.248666] FS-Cache: Duplicate cookie detected
[  +0.001228] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.001142] FS-Cache: O-cookie d=0000000084819ec0{9P.session} n=000000007a76c467
[  +0.002019] FS-Cache: O-key=[10] '34323934393337373437'
[  +0.001617] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.001420] FS-Cache: N-cookie d=0000000084819ec0{9P.session} n=0000000030ca9242
[  +0.001496] FS-Cache: N-key=[10] '34323934393337373437'
[  +0.101615] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000005]  failed 2
[  +0.010611] FS-Cache: Duplicate cookie detected
[  +0.000821] FS-Cache: O-cookie c=0000000d [p=00000002 fl=222 nc=0 na=1]
[  +0.000701] FS-Cache: O-cookie d=0000000084819ec0{9P.session} n=00000000a99f339d
[  +0.000988] FS-Cache: O-key=[10] '34323934393337373539'
[  +0.000706] FS-Cache: N-cookie c=0000000e [p=00000002 fl=2 nc=0 na=1]
[  +0.000922] FS-Cache: N-cookie d=0000000084819ec0{9P.session} n=00000000c801b6b5
[  +0.000994] FS-Cache: N-key=[10] '34323934393337373539'
[  +0.019128] WSL (1) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.210785] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.008780] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000922] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000676] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000776] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.004399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000749] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000863] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000729] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.944603] netlink: 'init': attribute type 4 has an invalid length.
[Dec27 14:31] tmpfs: Unknown parameter 'noswap'


==> etcd [6b462c93e61b] <==
{"level":"warn","ts":"2024-11-25T07:17:06.904910Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-25T07:17:06.906204Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-11-25T07:17:06.906257Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-11-25T07:17:06.906273Z","caller":"embed/config.go:687","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-11-25T07:17:06.906293Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-25T07:17:06.906325Z","caller":"embed/etcd.go:496","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-25T07:17:06.909258Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-11-25T07:17:06.909642Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-11-25T07:17:06.922577Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"12.553541ms"}
{"level":"info","ts":"2024-11-25T07:17:06.929770Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-11-25T07:17:06.934403Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":1230}
{"level":"info","ts":"2024-11-25T07:17:06.934505Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-11-25T07:17:06.935552Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2024-11-25T07:17:06.935571Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 1230, applied: 0, lastindex: 1230, lastterm: 2]"}
{"level":"warn","ts":"2024-11-25T07:17:06.938188Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-11-25T07:17:06.940719Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":668}
{"level":"info","ts":"2024-11-25T07:17:06.941823Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":1066}
{"level":"info","ts":"2024-11-25T07:17:06.945742Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-11-25T07:17:06.950179Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-11-25T07:17:06.950431Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-11-25T07:17:06.950480Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-11-25T07:17:06.950601Z","caller":"etcdserver/server.go:767","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-11-25T07:17:06.950691Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-25T07:17:06.950760Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-25T07:17:06.950769Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-11-25T07:17:06.950923Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-11-25T07:17:06.951038Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-11-25T07:17:06.951173Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-25T07:17:06.951223Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-11-25T07:17:06.950847Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-25T07:17:06.953725Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-11-25T07:17:06.953849Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-25T07:17:06.953865Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-11-25T07:17:06.953906Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-11-25T07:17:06.953925Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-11-25T07:17:08.136426Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2024-11-25T07:17:08.136475Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2024-11-25T07:17:08.136532Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2024-11-25T07:17:08.136542Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2024-11-25T07:17:08.136547Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-11-25T07:17:08.136558Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2024-11-25T07:17:08.136563Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-11-25T07:17:08.141767Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-11-25T07:17:08.141788Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-25T07:17:08.141805Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-11-25T07:17:08.142197Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-11-25T07:17:08.142233Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-11-25T07:17:08.144207Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-25T07:17:08.144414Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-11-25T07:17:08.144808Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-11-25T07:17:08.145809Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-11-25T07:27:08.149693Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1424}
{"level":"info","ts":"2024-11-25T07:27:08.177391Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1424,"took":"27.542854ms","hash":3874568539,"current-db-size-bytes":2449408,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1347584,"current-db-size-in-use":"1.3 MB"}
{"level":"info","ts":"2024-11-25T07:27:08.177475Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3874568539,"revision":1424,"compact-revision":668}
{"level":"info","ts":"2024-11-25T07:32:08.154919Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1662}
{"level":"info","ts":"2024-11-25T07:32:08.160744Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":1662,"took":"5.61421ms","hash":3807000764,"current-db-size-bytes":2449408,"current-db-size":"2.4 MB","current-db-size-in-use-bytes":1806336,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-11-25T07:32:08.160788Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3807000764,"revision":1662,"compact-revision":1424}


==> etcd [e373a5ba3131] <==
{"level":"info","ts":"2024-12-27T14:31:50.206127Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-12-27T14:31:50.206390Z","caller":"embed/etcd.go:310","msg":"starting an etcd server","etcd-version":"3.5.15","git-sha":"9a5533382","go-version":"go1.21.12","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-12-27T14:31:50.246466Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"39.807211ms"}
{"level":"info","ts":"2024-12-27T14:31:50.262144Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-12-27T14:31:50.274103Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2458}
{"level":"info","ts":"2024-12-27T14:31:50.275326Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-12-27T14:31:50.275407Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2024-12-27T14:31:50.275425Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 2458, applied: 0, lastindex: 2458, lastterm: 3]"}
{"level":"warn","ts":"2024-12-27T14:31:50.279981Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-12-27T14:31:50.283734Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1662}
{"level":"info","ts":"2024-12-27T14:31:50.286603Z","caller":"mvcc/kvstore.go:418","msg":"kvstore restored","current-rev":2066}
{"level":"info","ts":"2024-12-27T14:31:50.290801Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-12-27T14:31:50.294828Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-12-27T14:31:50.295203Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-12-27T14:31:50.295277Z","caller":"etcdserver/server.go:867","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.15","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-12-27T14:31:50.296035Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-27T14:31:50.296614Z","caller":"etcdserver/server.go:767","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-12-27T14:31:50.296767Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-27T14:31:50.296796Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-12-27T14:31:50.296808Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-12-27T14:31:50.296905Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-12-27T14:31:50.296079Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-27T14:31:50.297016Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-27T14:31:50.297059Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-12-27T14:31:50.300272Z","caller":"embed/etcd.go:599","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-27T14:31:50.300324Z","caller":"embed/etcd.go:571","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-12-27T14:31:50.301052Z","caller":"embed/etcd.go:728","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-12-27T14:31:50.301392Z","caller":"embed/etcd.go:279","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-12-27T14:31:50.301437Z","caller":"embed/etcd.go:870","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-12-27T14:31:51.976906Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2024-12-27T14:31:51.977061Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2024-12-27T14:31:51.977114Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-12-27T14:31:51.977136Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2024-12-27T14:31:51.977147Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-12-27T14:31:51.977162Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2024-12-27T14:31:51.977175Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-12-27T14:31:51.995967Z","caller":"etcdserver/server.go:2118","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-12-27T14:31:51.996041Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-27T14:31:51.996489Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-12-27T14:31:51.997112Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-12-27T14:31:51.997241Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-12-27T14:31:52.002391Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-27T14:31:52.002424Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2024-12-27T14:31:52.003498Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-12-27T14:31:52.003764Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-12-27T14:41:52.010523Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2439}
{"level":"info","ts":"2024-12-27T14:41:52.039810Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2439,"took":"28.777722ms","hash":3848105097,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1585152,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2024-12-27T14:41:52.039870Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":3848105097,"revision":2439,"compact-revision":1662}
{"level":"info","ts":"2024-12-27T14:46:52.016750Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2677}
{"level":"info","ts":"2024-12-27T14:46:52.022960Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2677,"took":"5.928944ms","hash":4077942858,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1765376,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2024-12-27T14:46:52.023031Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4077942858,"revision":2677,"compact-revision":2439}
{"level":"info","ts":"2024-12-27T14:51:52.016060Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2916}
{"level":"info","ts":"2024-12-27T14:51:52.025042Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":2916,"took":"8.569167ms","hash":1198107117,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1748992,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2024-12-27T14:51:52.025178Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1198107117,"revision":2916,"compact-revision":2677}
{"level":"info","ts":"2024-12-27T14:56:52.024226Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3157}
{"level":"info","ts":"2024-12-27T14:56:52.032312Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":3157,"took":"7.534704ms","hash":4129431953,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":1921024,"current-db-size-in-use":"1.9 MB"}
{"level":"info","ts":"2024-12-27T14:56:52.032413Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":4129431953,"revision":3157,"compact-revision":2916}
{"level":"info","ts":"2024-12-27T15:01:52.031637Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":3432}
{"level":"info","ts":"2024-12-27T15:01:52.040132Z","caller":"mvcc/kvstore_compaction.go:69","msg":"finished scheduled compaction","compact-revision":3432,"took":"7.966248ms","hash":332929480,"current-db-size-bytes":3158016,"current-db-size":"3.2 MB","current-db-size-in-use-bytes":2068480,"current-db-size-in-use":"2.1 MB"}
{"level":"info","ts":"2024-12-27T15:01:52.040224Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":332929480,"revision":3432,"compact-revision":3157}


==> kernel <==
 15:06:43 up  2:41,  0 users,  load average: 0.11, 0.15, 0.16
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [01b88254faa4] <==
W1125 07:17:08.583843       1 genericapiserver.go:765] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I1125 07:17:08.801010       1 secure_serving.go:213] Serving securely on [::]:8443
I1125 07:17:08.801435       1 local_available_controller.go:156] Starting LocalAvailability controller
I1125 07:17:08.801462       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1125 07:17:08.801479       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1125 07:17:08.801611       1 aggregator.go:169] waiting for initial CRD sync...
I1125 07:17:08.801657       1 controller.go:78] Starting OpenAPI AggregationController
I1125 07:17:08.801701       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1125 07:17:08.801720       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1125 07:17:08.802459       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1125 07:17:08.806302       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1125 07:17:08.802460       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1125 07:17:08.806531       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1125 07:17:08.806578       1 controller.go:142] Starting OpenAPI controller
I1125 07:17:08.806606       1 controller.go:90] Starting OpenAPI V3 controller
I1125 07:17:08.806619       1 naming_controller.go:294] Starting NamingConditionController
I1125 07:17:08.806639       1 establishing_controller.go:81] Starting EstablishingController
I1125 07:17:08.806662       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1125 07:17:08.806676       1 crd_finalizer.go:269] Starting CRDFinalizer
I1125 07:17:08.802459       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1125 07:17:08.806705       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1125 07:17:08.802494       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1125 07:17:08.806666       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1125 07:17:08.805996       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1125 07:17:08.806113       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1125 07:17:08.807119       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1125 07:17:08.806228       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1125 07:17:08.806290       1 controller.go:119] Starting legacy_token_tracking_controller
I1125 07:17:08.807279       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1125 07:17:08.806671       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1125 07:17:08.802478       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1125 07:17:08.805752       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1125 07:17:08.807423       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1125 07:17:08.806533       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1125 07:17:08.806541       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1125 07:17:08.969506       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1125 07:17:08.969630       1 cache.go:39] Caches are synced for LocalAvailability controller
I1125 07:17:08.970223       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1125 07:17:08.970274       1 aggregator.go:171] initial CRD sync complete...
I1125 07:17:08.970293       1 autoregister_controller.go:144] Starting autoregister controller
I1125 07:17:08.970302       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1125 07:17:08.970312       1 cache.go:39] Caches are synced for autoregister controller
I1125 07:17:08.973147       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1125 07:17:08.970235       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1125 07:17:08.973590       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1125 07:17:08.974118       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1125 07:17:08.974190       1 shared_informer.go:320] Caches are synced for configmaps
I1125 07:17:08.976691       1 shared_informer.go:320] Caches are synced for node_authorizer
I1125 07:17:09.069899       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1125 07:17:09.070140       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1125 07:17:09.070217       1 policy_source.go:224] refreshing policies
E1125 07:17:09.075873       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1125 07:17:09.170525       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
E1125 07:17:09.180137       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 9acf205d-13a8-40ce-ad8c-64e4af9fdc4e, UID in object meta: "
I1125 07:17:09.723045       1 controller.go:615] quota admission added evaluator for: endpoints
I1125 07:17:09.810906       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1125 07:17:12.698228       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1125 07:28:10.663261       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1125 07:28:10.669059       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1125 07:28:21.247906       1 alloc.go:330] "allocated clusterIPs" service="default/ml-app-service" clusterIPs={"IPv4":"10.97.255.165"}


==> kube-apiserver [1e3d2cbf4aee] <==
I1227 14:31:52.785579       1 secure_serving.go:213] Serving securely on [::]:8443
I1227 14:31:52.785663       1 system_namespaces_controller.go:66] Starting system namespaces controller
I1227 14:31:52.785680       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I1227 14:31:52.785684       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I1227 14:31:52.786668       1 controller.go:119] Starting legacy_token_tracking_controller
I1227 14:31:52.786691       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I1227 14:31:52.786779       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I1227 14:31:52.786800       1 local_available_controller.go:156] Starting LocalAvailability controller
I1227 14:31:52.786803       1 cache.go:32] Waiting for caches to sync for LocalAvailability controller
I1227 14:31:52.786987       1 controller.go:80] Starting OpenAPI V3 AggregationController
I1227 14:31:52.787091       1 aggregator.go:169] waiting for initial CRD sync...
I1227 14:31:52.787103       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I1227 14:31:52.787107       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I1227 14:31:52.794348       1 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I1227 14:31:52.794375       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1227 14:31:52.794479       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1227 14:31:52.794720       1 gc_controller.go:78] Starting apiserver lease garbage collector
I1227 14:31:52.794787       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I1227 14:31:52.794894       1 controller.go:78] Starting OpenAPI AggregationController
I1227 14:31:52.794934       1 controller.go:142] Starting OpenAPI controller
I1227 14:31:52.795033       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I1227 14:31:52.795760       1 cluster_authentication_trust_controller.go:443] Starting cluster_authentication_trust_controller controller
I1227 14:31:52.795770       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I1227 14:31:52.795871       1 dynamic_cafile_content.go:160] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I1227 14:31:52.795941       1 dynamic_cafile_content.go:160] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I1227 14:31:52.797219       1 naming_controller.go:294] Starting NamingConditionController
I1227 14:31:52.797532       1 establishing_controller.go:81] Starting EstablishingController
I1227 14:31:52.797609       1 controller.go:90] Starting OpenAPI V3 controller
I1227 14:31:52.797689       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I1227 14:31:52.797716       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I1227 14:31:52.797729       1 crd_finalizer.go:269] Starting CRDFinalizer
I1227 14:31:52.799025       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I1227 14:31:52.799033       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I1227 14:31:52.841298       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1227 14:31:52.941333       1 shared_informer.go:320] Caches are synced for crd-autoregister
I1227 14:31:52.942303       1 aggregator.go:171] initial CRD sync complete...
I1227 14:31:52.942343       1 autoregister_controller.go:144] Starting autoregister controller
I1227 14:31:52.942356       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1227 14:31:52.942370       1 cache.go:39] Caches are synced for autoregister controller
I1227 14:31:52.942685       1 shared_informer.go:320] Caches are synced for configmaps
I1227 14:31:52.942767       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1227 14:31:52.943119       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1227 14:31:52.943137       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1227 14:31:52.943282       1 cache.go:39] Caches are synced for LocalAvailability controller
I1227 14:31:52.945586       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I1227 14:31:53.041653       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I1227 14:31:53.041784       1 policy_source.go:224] refreshing policies
I1227 14:31:53.043165       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1227 14:31:53.047005       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I1227 14:31:53.051721       1 handler_discovery.go:450] Starting ResourceDiscoveryManager
I1227 14:31:53.143416       1 shared_informer.go:320] Caches are synced for node_authorizer
E1227 14:31:53.156241       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I1227 14:31:53.849220       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1227 14:31:56.660499       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1227 14:31:56.848324       1 controller.go:615] quota admission added evaluator for: endpoints
I1227 14:31:56.848324       1 controller.go:615] quota admission added evaluator for: endpoints
E1227 14:32:03.354425       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 3b436c9d-cfb6-4d71-92eb-2f1acf9fa0bd, UID in object meta: "
I1227 14:56:11.605969       1 controller.go:615] quota admission added evaluator for: deployments.apps
I1227 14:56:11.612827       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I1227 14:56:25.072144       1 alloc.go:330] "allocated clusterIPs" service="default/django-service" clusterIPs={"IPv4":"10.96.150.224"}


==> kube-controller-manager [5549c79bad51] <==
I1227 14:31:56.547995       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I1227 14:31:56.548009       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I1227 14:31:56.548515       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1227 14:31:56.550453       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1227 14:31:56.550531       1 shared_informer.go:320] Caches are synced for deployment
I1227 14:31:56.551308       1 shared_informer.go:320] Caches are synced for ReplicationController
I1227 14:31:56.552236       1 shared_informer.go:320] Caches are synced for disruption
I1227 14:31:56.553815       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1227 14:31:56.558666       1 shared_informer.go:320] Caches are synced for endpoint
I1227 14:31:56.559629       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1227 14:31:56.561319       1 shared_informer.go:320] Caches are synced for attach detach
I1227 14:31:56.650095       1 shared_informer.go:320] Caches are synced for daemon sets
I1227 14:31:56.653686       1 shared_informer.go:320] Caches are synced for taint
I1227 14:31:56.653919       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1227 14:31:56.654016       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1227 14:31:56.654085       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1227 14:31:56.654172       1 shared_informer.go:320] Caches are synced for resource quota
I1227 14:31:56.663577       1 shared_informer.go:320] Caches are synced for resource quota
I1227 14:31:56.666287       1 shared_informer.go:320] Caches are synced for stateful set
I1227 14:31:56.869785       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="310.035307ms"
I1227 14:31:56.869902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="60.144Âµs"
I1227 14:31:56.943977       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="384.227344ms"
I1227 14:31:56.944127       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="384.110333ms"
I1227 14:31:56.944239       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="74.391Âµs"
I1227 14:31:56.944383       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="384.270358ms"
I1227 14:31:56.944360       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="94.726Âµs"
I1227 14:31:56.944614       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="162.718Âµs"
I1227 14:31:57.146944       1 shared_informer.go:320] Caches are synced for garbage collector
I1227 14:31:57.147121       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1227 14:31:57.159504       1 shared_informer.go:320] Caches are synced for garbage collector
I1227 14:31:57.544079       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="91.487259ms"
I1227 14:31:57.544307       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="49.596Âµs"
I1227 14:31:57.659185       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="12.061756ms"
I1227 14:31:57.659277       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="37.163Âµs"
I1227 14:31:57.664629       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="53.665Âµs"
I1227 14:31:59.690945       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="9.117625ms"
I1227 14:31:59.691032       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="30.186Âµs"
I1227 14:32:18.939351       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="10.769117ms"
I1227 14:32:18.939447       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="41.879Âµs"
I1227 14:32:23.441012       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="65.234Âµs"
I1227 14:32:26.387180       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="12.871452ms"
I1227 14:32:26.387296       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="51.654Âµs"
I1227 14:32:36.158163       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="9.042028ms"
I1227 14:32:36.158399       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="30.062Âµs"
I1227 14:36:59.827032       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 14:42:06.640838       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 14:47:13.486226       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 14:52:19.719098       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 14:56:11.656477       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="37.620282ms"
I1227 14:56:11.676421       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="19.878579ms"
I1227 14:56:11.676516       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="42.243Âµs"
I1227 14:56:11.688298       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="51.248Âµs"
I1227 14:57:26.174591       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 15:02:32.907802       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1227 15:02:33.602162       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="54.888Âµs"
I1227 15:02:43.609191       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="42.390033ms"
I1227 15:02:43.619373       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="10.105801ms"
I1227 15:02:43.619457       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="46.671Âµs"
I1227 15:02:43.620026       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="116.04Âµs"
I1227 15:02:43.628865       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/django-app-7754657c8b" duration="43.174Âµs"


==> kube-controller-manager [d975caf3b0e2] <==
I1125 07:17:12.293877       1 shared_informer.go:320] Caches are synced for ReplicaSet
I1125 07:17:12.293897       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I1125 07:17:12.293940       1 shared_informer.go:320] Caches are synced for crt configmap
I1125 07:17:12.295128       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I1125 07:17:12.295148       1 shared_informer.go:320] Caches are synced for endpoint
I1125 07:17:12.295137       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I1125 07:17:12.297136       1 shared_informer.go:320] Caches are synced for GC
I1125 07:17:12.298263       1 shared_informer.go:320] Caches are synced for stateful set
I1125 07:17:12.298323       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I1125 07:17:12.298376       1 shared_informer.go:320] Caches are synced for ephemeral
I1125 07:17:12.299675       1 shared_informer.go:320] Caches are synced for cronjob
I1125 07:17:12.300360       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I1125 07:17:12.300400       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I1125 07:17:12.300430       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I1125 07:17:12.300439       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I1125 07:17:12.318085       1 shared_informer.go:320] Caches are synced for TTL
I1125 07:17:12.326591       1 shared_informer.go:320] Caches are synced for endpoint_slice
I1125 07:17:12.342918       1 shared_informer.go:320] Caches are synced for disruption
I1125 07:17:12.343098       1 shared_informer.go:320] Caches are synced for expand
I1125 07:17:12.344448       1 shared_informer.go:320] Caches are synced for taint
I1125 07:17:12.344597       1 shared_informer.go:320] Caches are synced for job
I1125 07:17:12.344620       1 node_lifecycle_controller.go:1232] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1125 07:17:12.344718       1 shared_informer.go:320] Caches are synced for daemon sets
I1125 07:17:12.344747       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1125 07:17:12.344796       1 node_lifecycle_controller.go:1078] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1125 07:17:12.474467       1 shared_informer.go:320] Caches are synced for persistent volume
I1125 07:17:12.514250       1 shared_informer.go:320] Caches are synced for resource quota
I1125 07:17:12.548577       1 shared_informer.go:320] Caches are synced for resource quota
I1125 07:17:12.553735       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="259.804013ms"
I1125 07:17:12.554482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="36.533Âµs"
I1125 07:17:12.557530       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="263.606357ms"
I1125 07:17:12.557567       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="263.654634ms"
I1125 07:17:12.557639       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="44.61Âµs"
I1125 07:17:12.557678       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="14.467Âµs"
I1125 07:17:12.590911       1 shared_informer.go:320] Caches are synced for attach detach
I1125 07:17:12.834428       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="9.655772ms"
I1125 07:17:12.834504       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-c5db448b4" duration="37.15Âµs"
I1125 07:17:12.851512       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="8.761877ms"
I1125 07:17:12.851591       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="37.269Âµs"
I1125 07:17:12.959889       1 shared_informer.go:320] Caches are synced for garbage collector
I1125 07:17:13.004322       1 shared_informer.go:320] Caches are synced for garbage collector
I1125 07:17:13.004402       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I1125 07:17:34.093580       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="11.802868ms"
I1125 07:17:34.093666       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="40.07Âµs"
I1125 07:17:35.970508       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="42.781Âµs"
I1125 07:17:37.448778       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="8.553098ms"
I1125 07:17:37.448874       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-6f6b679f8f" duration="30.709Âµs"
I1125 07:17:49.252637       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="10.226232ms"
I1125 07:17:49.252726       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-695b96c756" duration="35.808Âµs"
I1125 07:22:14.850264       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1125 07:27:21.531143       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1125 07:28:10.692211       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="18.198491ms"
I1125 07:28:10.699831       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="7.574477ms"
I1125 07:28:10.699890       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="35.202Âµs"
I1125 07:28:10.699903       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="7.486Âµs"
I1125 07:28:10.714589       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="39.833Âµs"
I1125 07:28:45.735661       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="8.49129ms"
I1125 07:28:45.735929       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/ml-app-7dc5bb847b" duration="203.863Âµs"
I1125 07:29:13.912329       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"
I1125 07:34:20.378343       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [348f4ace8051] <==
E1125 07:17:12.014668       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1125 07:17:12.020490       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1125 07:17:12.032381       1 server_linux.go:66] "Using iptables proxy"
I1125 07:17:12.268327       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1125 07:17:12.268396       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1125 07:17:12.288967       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1125 07:17:12.289021       1 server_linux.go:169] "Using iptables Proxier"
I1125 07:17:12.291211       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1125 07:17:12.297231       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1125 07:17:12.302683       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1125 07:17:12.302787       1 server.go:483] "Version info" version="v1.31.0"
I1125 07:17:12.302811       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1125 07:17:12.304382       1 config.go:197] "Starting service config controller"
I1125 07:17:12.304461       1 config.go:104] "Starting endpoint slice config controller"
I1125 07:17:12.305235       1 config.go:326] "Starting node config controller"
I1125 07:17:12.305393       1 shared_informer.go:313] Waiting for caches to sync for service config
I1125 07:17:12.305395       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1125 07:17:12.305394       1 shared_informer.go:313] Waiting for caches to sync for node config
I1125 07:17:12.405626       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1125 07:17:12.405683       1 shared_informer.go:320] Caches are synced for service config
I1125 07:17:12.405691       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [8f79b33606b8] <==
E1227 14:31:56.866110       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E1227 14:31:56.873386       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I1227 14:31:56.964890       1 server_linux.go:66] "Using iptables proxy"
I1227 14:31:57.574669       1 server.go:677] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E1227 14:31:57.574792       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1227 14:31:57.678013       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1227 14:31:57.678069       1 server_linux.go:169] "Using iptables Proxier"
I1227 14:31:57.680631       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E1227 14:31:57.686143       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E1227 14:31:57.691335       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I1227 14:31:57.692784       1 server.go:483] "Version info" version="v1.31.0"
I1227 14:31:57.692831       1 server.go:485] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1227 14:31:57.696625       1 config.go:326] "Starting node config controller"
I1227 14:31:57.696658       1 config.go:104] "Starting endpoint slice config controller"
I1227 14:31:57.696676       1 config.go:197] "Starting service config controller"
I1227 14:31:57.697325       1 shared_informer.go:313] Waiting for caches to sync for node config
I1227 14:31:57.697326       1 shared_informer.go:313] Waiting for caches to sync for service config
I1227 14:31:57.697328       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I1227 14:31:57.798088       1 shared_informer.go:320] Caches are synced for endpoint slice config
I1227 14:31:57.798137       1 shared_informer.go:320] Caches are synced for service config
I1227 14:31:57.798176       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [7fc4527ec12f] <==
I1125 07:17:07.676045       1 serving.go:386] Generated self-signed cert in-memory
W1125 07:17:08.887787       1 requestheader_controller.go:196] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1125 07:17:08.887839       1 authentication.go:370] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found]
W1125 07:17:08.887851       1 authentication.go:371] Continuing without authentication configuration. This may treat all requests as anonymous.
W1125 07:17:08.887856       1 authentication.go:372] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1125 07:17:09.092320       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1125 07:17:09.092365       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1125 07:17:09.095022       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1125 07:17:09.095060       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1125 07:17:09.099034       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1125 07:17:09.169365       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1125 07:17:09.269707       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [f091bd1bd268] <==
I1227 14:31:50.595692       1 serving.go:386] Generated self-signed cert in-memory
I1227 14:31:53.159919       1 server.go:167] "Starting Kubernetes Scheduler" version="v1.31.0"
I1227 14:31:53.159953       1 server.go:169] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1227 14:31:53.166876       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1227 14:31:53.168041       1 requestheader_controller.go:172] Starting RequestHeaderAuthRequestController
I1227 14:31:53.168057       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1227 14:31:53.168082       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I1227 14:31:53.172267       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1227 14:31:53.172290       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1227 14:31:53.172363       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1227 14:31:53.172368       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1227 14:31:53.268295       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I1227 14:31:53.272907       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1227 14:31:53.272919       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file


==> kubelet <==
Dec 27 14:56:12 minikube kubelet[1726]: E1227 14:56:12.328792    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:13.328760394 +0000 UTC m=+1465.286232294 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:12 minikube kubelet[1726]: E1227 14:56:12.328981    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:13.328941384 +0000 UTC m=+1465.286413284 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:13 minikube kubelet[1726]: E1227 14:56:13.339347    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:15.339319679 +0000 UTC m=+1467.296791591 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:13 minikube kubelet[1726]: E1227 14:56:13.339518    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:15.33948423 +0000 UTC m=+1467.296956120 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:15 minikube kubelet[1726]: E1227 14:56:15.354983    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:19.354952768 +0000 UTC m=+1471.314556022 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:15 minikube kubelet[1726]: E1227 14:56:15.355070    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:19.35505866 +0000 UTC m=+1471.314661903 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:19 minikube kubelet[1726]: E1227 14:56:19.390906    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:27.390882561 +0000 UTC m=+1479.350485780 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:19 minikube kubelet[1726]: E1227 14:56:19.390971    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:27.390962671 +0000 UTC m=+1479.350565902 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:27 minikube kubelet[1726]: E1227 14:56:27.458265    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:43.458241394 +0000 UTC m=+1495.417844623 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:27 minikube kubelet[1726]: E1227 14:56:27.458334    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:56:43.458325429 +0000 UTC m=+1495.417928656 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:43 minikube kubelet[1726]: E1227 14:56:43.497361    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:57:15.49732901 +0000 UTC m=+1527.456932241 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:56:43 minikube kubelet[1726]: E1227 14:56:43.497470    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:57:15.497430188 +0000 UTC m=+1527.457033441 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:57:15 minikube kubelet[1726]: E1227 14:57:15.572414    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 14:58:19.57238216 +0000 UTC m=+1591.535401661 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:57:15 minikube kubelet[1726]: E1227 14:57:15.572626    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 14:58:19.572595068 +0000 UTC m=+1591.535614583 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:58:14 minikube kubelet[1726]: E1227 14:58:14.660633    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-wh24c" podUID="be1e9995-b17c-4435-8d27-627e3f102339"
Dec 27 14:58:14 minikube kubelet[1726]: E1227 14:58:14.661869    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-p5n2r" podUID="30ad8043-cb7f-49f3-8438-b94cc6a7e274"
Dec 27 14:58:19 minikube kubelet[1726]: E1227 14:58:19.619928    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 15:00:21.619893909 +0000 UTC m=+1713.586204681 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 14:58:19 minikube kubelet[1726]: E1227 14:58:19.619976    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 15:00:21.619972149 +0000 UTC m=+1713.586282922 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:00:21 minikube kubelet[1726]: E1227 15:00:21.702243    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:23.702216778 +0000 UTC m=+1835.674907412 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:00:21 minikube kubelet[1726]: E1227 15:00:21.702308    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:23.702304355 +0000 UTC m=+1835.674994990 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:00:28 minikube kubelet[1726]: E1227 15:00:28.369237    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-p5n2r" podUID="30ad8043-cb7f-49f3-8438-b94cc6a7e274"
Dec 27 15:00:31 minikube kubelet[1726]: E1227 15:00:31.368628    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-wh24c" podUID="be1e9995-b17c-4435-8d27-627e3f102339"
Dec 27 15:02:23 minikube kubelet[1726]: E1227 15:02:23.782612    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage podName:be1e9995-b17c-4435-8d27-627e3f102339 nodeName:}" failed. No retries permitted until 2024-12-27 15:04:25.782595506 +0000 UTC m=+1957.762182510 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage") pod "django-app-7754657c8b-wh24c" (UID: "be1e9995-b17c-4435-8d27-627e3f102339") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:23 minikube kubelet[1726]: E1227 15:02:23.782657    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage podName:30ad8043-cb7f-49f3-8438-b94cc6a7e274 nodeName:}" failed. No retries permitted until 2024-12-27 15:04:25.782652393 +0000 UTC m=+1957.762239395 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage") pod "django-app-7754657c8b-p5n2r" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:33 minikube kubelet[1726]: E1227 15:02:33.610861    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context canceled" pod="default/django-app-7754657c8b-p5n2r" podUID="30ad8043-cb7f-49f3-8438-b94cc6a7e274"
Dec 27 15:02:33 minikube kubelet[1726]: E1227 15:02:33.611953    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context canceled" pod="default/django-app-7754657c8b-wh24c" podUID="be1e9995-b17c-4435-8d27-627e3f102339"
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.769919    1726 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-8d5nz\" (UniqueName: \"kubernetes.io/projected/30ad8043-cb7f-49f3-8438-b94cc6a7e274-kube-api-access-8d5nz\") pod \"30ad8043-cb7f-49f3-8438-b94cc6a7e274\" (UID: \"30ad8043-cb7f-49f3-8438-b94cc6a7e274\") "
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.770057    1726 reconciler_common.go:159] "operationExecutor.UnmountVolume started for volume \"kube-api-access-247x7\" (UniqueName: \"kubernetes.io/projected/be1e9995-b17c-4435-8d27-627e3f102339-kube-api-access-247x7\") pod \"be1e9995-b17c-4435-8d27-627e3f102339\" (UID: \"be1e9995-b17c-4435-8d27-627e3f102339\") "
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.775713    1726 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/be1e9995-b17c-4435-8d27-627e3f102339-kube-api-access-247x7" (OuterVolumeSpecName: "kube-api-access-247x7") pod "be1e9995-b17c-4435-8d27-627e3f102339" (UID: "be1e9995-b17c-4435-8d27-627e3f102339"). InnerVolumeSpecName "kube-api-access-247x7". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.775722    1726 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/30ad8043-cb7f-49f3-8438-b94cc6a7e274-kube-api-access-8d5nz" (OuterVolumeSpecName: "kube-api-access-8d5nz") pod "30ad8043-cb7f-49f3-8438-b94cc6a7e274" (UID: "30ad8043-cb7f-49f3-8438-b94cc6a7e274"). InnerVolumeSpecName "kube-api-access-8d5nz". PluginName "kubernetes.io/projected", VolumeGidValue ""
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.871618    1726 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-8d5nz\" (UniqueName: \"kubernetes.io/projected/30ad8043-cb7f-49f3-8438-b94cc6a7e274-kube-api-access-8d5nz\") on node \"minikube\" DevicePath \"\""
Dec 27 15:02:33 minikube kubelet[1726]: I1227 15:02:33.871718    1726 reconciler_common.go:288] "Volume detached for volume \"kube-api-access-247x7\" (UniqueName: \"kubernetes.io/projected/be1e9995-b17c-4435-8d27-627e3f102339-kube-api-access-247x7\") on node \"minikube\" DevicePath \"\""
Dec 27 15:02:34 minikube kubelet[1726]: I1227 15:02:34.780508    1726 reconciler_common.go:288] "Volume detached for volume \"sqlite-storage\" (UniqueName: \"kubernetes.io/host-path/be1e9995-b17c-4435-8d27-627e3f102339-sqlite-storage\") on node \"minikube\" DevicePath \"\""
Dec 27 15:02:34 minikube kubelet[1726]: I1227 15:02:34.780620    1726 reconciler_common.go:288] "Volume detached for volume \"sqlite-storage\" (UniqueName: \"kubernetes.io/host-path/30ad8043-cb7f-49f3-8438-b94cc6a7e274-sqlite-storage\") on node \"minikube\" DevicePath \"\""
Dec 27 15:02:36 minikube kubelet[1726]: I1227 15:02:36.370780    1726 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="30ad8043-cb7f-49f3-8438-b94cc6a7e274" path="/var/lib/kubelet/pods/30ad8043-cb7f-49f3-8438-b94cc6a7e274/volumes"
Dec 27 15:02:36 minikube kubelet[1726]: I1227 15:02:36.371129    1726 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="be1e9995-b17c-4435-8d27-627e3f102339" path="/var/lib/kubelet/pods/be1e9995-b17c-4435-8d27-627e3f102339/volumes"
Dec 27 15:02:43 minikube kubelet[1726]: I1227 15:02:43.654646    1726 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sqlite-storage\" (UniqueName: \"kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage\") pod \"django-app-7754657c8b-z5ckw\" (UID: \"d3f5b350-db2f-4beb-bc0c-5401bae00445\") " pod="default/django-app-7754657c8b-z5ckw"
Dec 27 15:02:43 minikube kubelet[1726]: I1227 15:02:43.654764    1726 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"sqlite-storage\" (UniqueName: \"kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage\") pod \"django-app-7754657c8b-7xp9r\" (UID: \"571e5a7d-9160-49d5-99a5-0d8e1dd635ec\") " pod="default/django-app-7754657c8b-7xp9r"
Dec 27 15:02:43 minikube kubelet[1726]: I1227 15:02:43.654906    1726 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qqchz\" (UniqueName: \"kubernetes.io/projected/d3f5b350-db2f-4beb-bc0c-5401bae00445-kube-api-access-qqchz\") pod \"django-app-7754657c8b-z5ckw\" (UID: \"d3f5b350-db2f-4beb-bc0c-5401bae00445\") " pod="default/django-app-7754657c8b-z5ckw"
Dec 27 15:02:43 minikube kubelet[1726]: I1227 15:02:43.654949    1726 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sjgnk\" (UniqueName: \"kubernetes.io/projected/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-kube-api-access-sjgnk\") pod \"django-app-7754657c8b-7xp9r\" (UID: \"571e5a7d-9160-49d5-99a5-0d8e1dd635ec\") " pod="default/django-app-7754657c8b-7xp9r"
Dec 27 15:02:43 minikube kubelet[1726]: E1227 15:02:43.756589    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:02:44.256561769 +0000 UTC m=+1856.236148780 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:43 minikube kubelet[1726]: E1227 15:02:43.756769    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:44.256745428 +0000 UTC m=+1856.236332439 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:44 minikube kubelet[1726]: E1227 15:02:44.261423    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:02:45.261388359 +0000 UTC m=+1857.240975387 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:44 minikube kubelet[1726]: E1227 15:02:44.261628    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:45.261591217 +0000 UTC m=+1857.241178272 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:45 minikube kubelet[1726]: E1227 15:02:45.266821    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:02:47.266789428 +0000 UTC m=+1859.248180079 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:45 minikube kubelet[1726]: E1227 15:02:45.266946    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:47.266908743 +0000 UTC m=+1859.248299410 (durationBeforeRetry 2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:47 minikube kubelet[1726]: E1227 15:02:47.281245    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:02:51.28121591 +0000 UTC m=+1863.262606553 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:47 minikube kubelet[1726]: E1227 15:02:47.281355    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:51.281320265 +0000 UTC m=+1863.262710908 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:51 minikube kubelet[1726]: E1227 15:02:51.312885    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:02:59.31286349 +0000 UTC m=+1871.294254111 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:51 minikube kubelet[1726]: E1227 15:02:51.312940    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:02:59.312933521 +0000 UTC m=+1871.294324156 (durationBeforeRetry 8s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:59 minikube kubelet[1726]: E1227 15:02:59.385492    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:03:15.385462122 +0000 UTC m=+1887.366852773 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:02:59 minikube kubelet[1726]: E1227 15:02:59.385758    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:03:15.385720454 +0000 UTC m=+1887.367111101 (durationBeforeRetry 16s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:03:15 minikube kubelet[1726]: E1227 15:03:15.422971    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:03:47.422942795 +0000 UTC m=+1919.405524454 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:03:15 minikube kubelet[1726]: E1227 15:03:15.423083    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:03:47.423051378 +0000 UTC m=+1919.405633059 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:03:47 minikube kubelet[1726]: E1227 15:03:47.495069    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:04:51.495048922 +0000 UTC m=+1983.479643202 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:03:47 minikube kubelet[1726]: E1227 15:03:47.495114    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:04:51.495110156 +0000 UTC m=+1983.479704438 (durationBeforeRetry 1m4s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:04:46 minikube kubelet[1726]: E1227 15:04:46.592943    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-z5ckw" podUID="d3f5b350-db2f-4beb-bc0c-5401bae00445"
Dec 27 15:04:46 minikube kubelet[1726]: E1227 15:04:46.601716    1726 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[sqlite-storage], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="default/django-app-7754657c8b-7xp9r" podUID="571e5a7d-9160-49d5-99a5-0d8e1dd635ec"
Dec 27 15:04:51 minikube kubelet[1726]: E1227 15:04:51.561687    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage podName:571e5a7d-9160-49d5-99a5-0d8e1dd635ec nodeName:}" failed. No retries permitted until 2024-12-27 15:06:53.561661422 +0000 UTC m=+2105.550042540 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/571e5a7d-9160-49d5-99a5-0d8e1dd635ec-sqlite-storage") pod "django-app-7754657c8b-7xp9r" (UID: "571e5a7d-9160-49d5-99a5-0d8e1dd635ec") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file
Dec 27 15:04:51 minikube kubelet[1726]: E1227 15:04:51.561757    1726 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage podName:d3f5b350-db2f-4beb-bc0c-5401bae00445 nodeName:}" failed. No retries permitted until 2024-12-27 15:06:53.56174851 +0000 UTC m=+2105.550129626 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "sqlite-storage" (UniqueName: "kubernetes.io/host-path/d3f5b350-db2f-4beb-bc0c-5401bae00445-sqlite-storage") pod "django-app-7754657c8b-z5ckw" (UID: "d3f5b350-db2f-4beb-bc0c-5401bae00445") : hostPath type check failed: D:/Farm_ibm/Farm-Product-E-Commerce/far_pro/db.sqlite3 is not a file


==> kubernetes-dashboard [241c2ac823bb] <==
2024/12/27 14:31:57 Starting overwatch
panic: Get "https://10.96.0.1:443/api/v1/namespaces/kubernetes-dashboard/secrets/kubernetes-dashboard-csrf": dial tcp 10.96.0.1:443: connect: connection refused

goroutine 1 [running]:
github.com/kubernetes/dashboard/src/app/backend/client/csrf.(*csrfTokenManager).init(0xc00071fae8)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:41 +0x30e
github.com/kubernetes/dashboard/src/app/backend/client/csrf.NewCsrfTokenManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/csrf/manager.go:66
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).initCSRFKey(0xc00050e100)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:527 +0x94
github.com/kubernetes/dashboard/src/app/backend/client.(*clientManager).init(0x19aba3a?)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:495 +0x32
github.com/kubernetes/dashboard/src/app/backend/client.NewClientManager(...)
	/home/runner/work/dashboard/dashboard/src/app/backend/client/manager.go:594
main.main()
	/home/runner/work/dashboard/dashboard/src/app/backend/dashboard.go:96 +0x1cf
2024/12/27 14:31:57 Using namespace: kubernetes-dashboard
2024/12/27 14:31:57 Using in-cluster config to connect to apiserver
2024/12/27 14:31:57 Using secret token for csrf signing
2024/12/27 14:31:57 Initializing csrf token from kubernetes-dashboard-csrf secret


==> kubernetes-dashboard [c6ca97adf2cd] <==
2024/12/27 14:32:35 Starting overwatch
2024/12/27 14:32:35 Using namespace: kubernetes-dashboard
2024/12/27 14:32:35 Using in-cluster config to connect to apiserver
2024/12/27 14:32:35 Using secret token for csrf signing
2024/12/27 14:32:35 Initializing csrf token from kubernetes-dashboard-csrf secret
2024/12/27 14:32:35 Empty token. Generating and storing in a secret kubernetes-dashboard-csrf
2024/12/27 14:32:35 Successful initial request to the apiserver, version: v1.31.0
2024/12/27 14:32:35 Generating JWE encryption key
2024/12/27 14:32:35 New synchronizer has been registered: kubernetes-dashboard-key-holder-kubernetes-dashboard. Starting
2024/12/27 14:32:35 Starting secret synchronizer for kubernetes-dashboard-key-holder in namespace kubernetes-dashboard
2024/12/27 14:32:35 Initializing JWE encryption key from synchronized object
2024/12/27 14:32:35 Creating in-cluster Sidecar client
2024/12/27 14:32:35 Successful request to sidecar
2024/12/27 14:32:35 Serving insecurely on HTTP port: 9090


==> storage-provisioner [4c61eebac961] <==
I1227 14:31:56.653342       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1227 14:32:17.719975       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [b8a793709c23] <==
I1227 14:32:31.639820       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1227 14:32:31.648476       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1227 14:32:31.648904       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1227 14:32:49.047227       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1227 14:32:49.047387       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2b127357-0ab1-4f0a-9c2c-a6757b717680", APIVersion:"v1", ResourceVersion:"2246", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_e26c1389-6c87-425f-9d98-2706ff9d1d05 became leader
I1227 14:32:49.047426       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_e26c1389-6c87-425f-9d98-2706ff9d1d05!
I1227 14:32:49.148186       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_e26c1389-6c87-425f-9d98-2706ff9d1d05!

